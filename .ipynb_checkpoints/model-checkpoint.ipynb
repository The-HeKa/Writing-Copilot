{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-02 11:52:41.647110: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myuanlu\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from simpletransformers.classification import MultiLabelClassificationModel, MultiLabelClassificationArgs\n",
    "from simpletransformers.t5 import T5Model, T5Args\n",
    "from simpletransformers.seq2seq import Seq2SeqModel, Seq2SeqArgs\n",
    "from sklearn.utils import shuffle\n",
    "import pandas as pd\n",
    "import logging\n",
    "import os\n",
    "import csv\n",
    "import wandb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_1673/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">546452869.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">108</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_1673/546452869.py'</span>                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_1673/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">546452869.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">55</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__init__</span>                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_1673/546452869.py'</span>                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_1673/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">546452869.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">92</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">mask_train</span>                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_1673/546452869.py'</span>                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/yuan/anaconda3/envs/wtcpt/lib/python3.8/site-packages/simpletransformers/seq2seq/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">seq2seq_m</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">odel.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">313</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__init__</span>                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 310 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> encoder_decoder_type <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> [<span style=\"color: #808000; text-decoration-color: #808000\">\"bart\"</span>, <span style=\"color: #808000; text-decoration-color: #808000\">\"mbart\"</span>, <span style=\"color: #808000; text-decoration-color: #808000\">\"marian\"</span>]:                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 311 │   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.model = model_class.from_pretrained(encoder_decoder_name)            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 312 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> encoder_decoder_type <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> [<span style=\"color: #808000; text-decoration-color: #808000\">\"bart\"</span>, <span style=\"color: #808000; text-decoration-color: #808000\">\"mbart\"</span>]:                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 313 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.encoder_tokenizer = tokenizer_class.from_pretrained(             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 314 │   │   │   │   │   │   </span>encoder_decoder_name                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 315 │   │   │   │   │   </span>)                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 316 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> encoder_decoder_type == <span style=\"color: #808000; text-decoration-color: #808000\">\"marian\"</span>:                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/yuan/anaconda3/envs/wtcpt/lib/python3.8/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">tokenization_utils_base</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1777</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">from_pretrained</span>                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1774 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1775 │   │   │   │   </span>logger.info(<span style=\"color: #808000; text-decoration-color: #808000\">f\"loading file {</span>file_path<span style=\"color: #808000; text-decoration-color: #808000\">} from cache at {</span>resolved_vocab_fil  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1776 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1777 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">cls</span>._from_pretrained(                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1778 │   │   │   </span>resolved_vocab_files,                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1779 │   │   │   </span>pretrained_model_name_or_path,                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1780 │   │   │   </span>init_configuration,                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/yuan/anaconda3/envs/wtcpt/lib/python3.8/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">tokenization_utils_base</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1807</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_from_pretrained</span>                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1804 │   │   </span>from_slow = kwargs.get(<span style=\"color: #808000; text-decoration-color: #808000\">\"from_slow\"</span>, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">False</span>)                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1805 │   │   </span>has_tokenizer_file = resolved_vocab_files.get(<span style=\"color: #808000; text-decoration-color: #808000\">\"tokenizer_file\"</span>, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>) <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">Non</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1806 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> (from_slow <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> has_tokenizer_file) <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">and</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">cls</span>.slow_tokenizer_class <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">Non</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1807 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>slow_tokenizer = (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">cls</span>.slow_tokenizer_class)._from_pretrained(                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1808 │   │   │   │   </span>copy.deepcopy(resolved_vocab_files),                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1809 │   │   │   │   </span>pretrained_model_name_or_path,                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1810 │   │   │   │   </span>copy.deepcopy(init_configuration),                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/yuan/anaconda3/envs/wtcpt/lib/python3.8/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">tokenization_utils_base</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1932</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_from_pretrained</span>                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1929 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1930 │   │   # Instantiate tokenizer.</span>                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1931 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1932 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>tokenizer = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">cls</span>(*init_inputs, **init_kwargs)                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1933 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">except</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">OSError</span>:                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1934 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">OSError</span>(                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1935 │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"Unable to load vocabulary from file. \"</span>                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/yuan/anaconda3/envs/wtcpt/lib/python3.8/site-packages/transformers/models/bart/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">tokenizatio</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">n_bart.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">220</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__init__</span>                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">217 │   │   │   </span>**kwargs,                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">218 │   │   </span>)                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">219 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>220 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">open</span>(vocab_file, encoding=<span style=\"color: #808000; text-decoration-color: #808000\">\"utf-8\"</span>) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">as</span> vocab_handle:                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">221 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.encoder = json.load(vocab_handle)                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">222 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.decoder = {v: k <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> k, v <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.encoder.items()}                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">223 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.errors = errors  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># how to handle errors in decoding</span>                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">TypeError: </span>expected str, bytes or os.PathLike object, not NoneType\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_1673/\u001b[0m\u001b[1;33m546452869.py\u001b[0m:\u001b[94m108\u001b[0m in \u001b[92m<module>\u001b[0m                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_1673/546452869.py'\u001b[0m                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_1673/\u001b[0m\u001b[1;33m546452869.py\u001b[0m:\u001b[94m55\u001b[0m in \u001b[92m__init__\u001b[0m                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_1673/546452869.py'\u001b[0m                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_1673/\u001b[0m\u001b[1;33m546452869.py\u001b[0m:\u001b[94m92\u001b[0m in \u001b[92mmask_train\u001b[0m                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_1673/546452869.py'\u001b[0m                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/yuan/anaconda3/envs/wtcpt/lib/python3.8/site-packages/simpletransformers/seq2seq/\u001b[0m\u001b[1;33mseq2seq_m\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33model.py\u001b[0m:\u001b[94m313\u001b[0m in \u001b[92m__init__\u001b[0m                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 310 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m encoder_decoder_type \u001b[95min\u001b[0m [\u001b[33m\"\u001b[0m\u001b[33mbart\u001b[0m\u001b[33m\"\u001b[0m, \u001b[33m\"\u001b[0m\u001b[33mmbart\u001b[0m\u001b[33m\"\u001b[0m, \u001b[33m\"\u001b[0m\u001b[33mmarian\u001b[0m\u001b[33m\"\u001b[0m]:                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 311 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.model = model_class.from_pretrained(encoder_decoder_name)            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 312 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m encoder_decoder_type \u001b[95min\u001b[0m [\u001b[33m\"\u001b[0m\u001b[33mbart\u001b[0m\u001b[33m\"\u001b[0m, \u001b[33m\"\u001b[0m\u001b[33mmbart\u001b[0m\u001b[33m\"\u001b[0m]:                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 313 \u001b[2m│   │   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.encoder_tokenizer = tokenizer_class.from_pretrained(             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 314 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0mencoder_decoder_name                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 315 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m)                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 316 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94melif\u001b[0m encoder_decoder_type == \u001b[33m\"\u001b[0m\u001b[33mmarian\u001b[0m\u001b[33m\"\u001b[0m:                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/yuan/anaconda3/envs/wtcpt/lib/python3.8/site-packages/transformers/\u001b[0m\u001b[1;33mtokenization_utils_base\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33m.py\u001b[0m:\u001b[94m1777\u001b[0m in \u001b[92mfrom_pretrained\u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1774 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1775 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mlogger.info(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mloading file \u001b[0m\u001b[33m{\u001b[0mfile_path\u001b[33m}\u001b[0m\u001b[33m from cache at \u001b[0m\u001b[33m{\u001b[0mresolved_vocab_fil  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1776 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1777 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mcls\u001b[0m._from_pretrained(                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1778 \u001b[0m\u001b[2m│   │   │   \u001b[0mresolved_vocab_files,                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1779 \u001b[0m\u001b[2m│   │   │   \u001b[0mpretrained_model_name_or_path,                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1780 \u001b[0m\u001b[2m│   │   │   \u001b[0minit_configuration,                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/yuan/anaconda3/envs/wtcpt/lib/python3.8/site-packages/transformers/\u001b[0m\u001b[1;33mtokenization_utils_base\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33m.py\u001b[0m:\u001b[94m1807\u001b[0m in \u001b[92m_from_pretrained\u001b[0m                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1804 \u001b[0m\u001b[2m│   │   \u001b[0mfrom_slow = kwargs.get(\u001b[33m\"\u001b[0m\u001b[33mfrom_slow\u001b[0m\u001b[33m\"\u001b[0m, \u001b[94mFalse\u001b[0m)                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1805 \u001b[0m\u001b[2m│   │   \u001b[0mhas_tokenizer_file = resolved_vocab_files.get(\u001b[33m\"\u001b[0m\u001b[33mtokenizer_file\u001b[0m\u001b[33m\"\u001b[0m, \u001b[94mNone\u001b[0m) \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNon\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1806 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m (from_slow \u001b[95mor\u001b[0m \u001b[95mnot\u001b[0m has_tokenizer_file) \u001b[95mand\u001b[0m \u001b[96mcls\u001b[0m.slow_tokenizer_class \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNon\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1807 \u001b[2m│   │   │   \u001b[0mslow_tokenizer = (\u001b[96mcls\u001b[0m.slow_tokenizer_class)._from_pretrained(                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1808 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mcopy.deepcopy(resolved_vocab_files),                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1809 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mpretrained_model_name_or_path,                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1810 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mcopy.deepcopy(init_configuration),                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/yuan/anaconda3/envs/wtcpt/lib/python3.8/site-packages/transformers/\u001b[0m\u001b[1;33mtokenization_utils_base\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33m.py\u001b[0m:\u001b[94m1932\u001b[0m in \u001b[92m_from_pretrained\u001b[0m                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1929 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1930 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Instantiate tokenizer.\u001b[0m                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1931 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1932 \u001b[2m│   │   │   \u001b[0mtokenizer = \u001b[96mcls\u001b[0m(*init_inputs, **init_kwargs)                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1933 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mexcept\u001b[0m \u001b[96mOSError\u001b[0m:                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1934 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mOSError\u001b[0m(                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1935 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mUnable to load vocabulary from file. \u001b[0m\u001b[33m\"\u001b[0m                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/yuan/anaconda3/envs/wtcpt/lib/python3.8/site-packages/transformers/models/bart/\u001b[0m\u001b[1;33mtokenizatio\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33mn_bart.py\u001b[0m:\u001b[94m220\u001b[0m in \u001b[92m__init__\u001b[0m                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m217 \u001b[0m\u001b[2m│   │   │   \u001b[0m**kwargs,                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m218 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m219 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m220 \u001b[2m│   │   \u001b[0m\u001b[94mwith\u001b[0m \u001b[96mopen\u001b[0m(vocab_file, encoding=\u001b[33m\"\u001b[0m\u001b[33mutf-8\u001b[0m\u001b[33m\"\u001b[0m) \u001b[94mas\u001b[0m vocab_handle:                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m221 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.encoder = json.load(vocab_handle)                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m222 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.decoder = {v: k \u001b[94mfor\u001b[0m k, v \u001b[95min\u001b[0m \u001b[96mself\u001b[0m.encoder.items()}                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m223 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.errors = errors  \u001b[2m# how to handle errors in decoding\u001b[0m                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mTypeError: \u001b[0mexpected str, bytes or os.PathLike object, not NoneType\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "folder_name = 'chenyu_51657'\n",
    "\n",
    "class data:\n",
    "    def __init__(self, file_name) -> None:\n",
    "\n",
    "        self.task_type = folder_name.split('_')[0]\n",
    "        self.file_name = file_name.replace('.csv','')\n",
    "        self.file_path = folder_name+'/'+self.file_name+'.csv'\n",
    "        self.type = self.file_name.split('_')[0]\n",
    "\n",
    "        self.random_state = 96\n",
    "        self.max_sample = 5000\n",
    "\n",
    "        if self.type == 'label':\n",
    "\n",
    "            self.p = float(self.file_name.split('_')[1])\n",
    "            self.len = int(self.file_name.split('_')[2])\n",
    "\n",
    "            with open(self.file_path, mode = 'r') as f:\n",
    "                self.data = pd.DataFrame([[i[0],list([int(r) for r in i[1]])] for i in csv.reader(f)])\n",
    "                self.data.columns = ['text', 'labels']\n",
    "            \n",
    "            self.wandb_job = '{}_{}'.format(self.task_type, self.file_name)\n",
    "            self.output_dir = 'outputs/'+self.task_type+'/'+self.file_name+'/'\n",
    "\n",
    "            self.data = shuffle(self.data, random_state=self.random_state)\n",
    "            self.data.reset_index(inplace=True, drop=True)\n",
    "            self.data = self.data[:self.max_sample]\n",
    "\n",
    "            self.train, self.test = train_test_split(self.data, test_size=0.2, random_state=self.random_state)\n",
    "            del self.data\n",
    "\n",
    "            self.label_train()\n",
    "              \n",
    "        elif self.type == 'mask':\n",
    "\n",
    "            self.p = 1.0\n",
    "            self.len = int(self.file_name.split('_')[1])\n",
    "\n",
    "            with open(self.file_path, mode = 'r') as f:\n",
    "                self.data = pd.DataFrame([[str(i[0]), str(i[1])] for i in csv.reader(f)]).astype('string')\n",
    "                self.data.columns = [\"input_text\", \"target_text\"]\n",
    "            \n",
    "            self.wandb_job = '{}_{}'.format(self.task_type, self.file_name)\n",
    "            self.output_dir = 'outputs/'+self.task_type+'/'+self.file_name+'/'\n",
    "            \n",
    "            self.data = shuffle(self.data, random_state=self.random_state)\n",
    "            self.data.reset_index(inplace=True, drop=True)\n",
    "            self.data = self.data[:self.max_sample]\n",
    "\n",
    "            self.train, self.test = train_test_split(self.data, test_size=0.2, random_state=self.random_state)\n",
    "            self.train, self.eval = train_test_split(self.train, test_size=0.2, random_state=self.random_state)\n",
    "            del self.data\n",
    "\n",
    "            self.mask_train()\n",
    "    \n",
    "\n",
    "    def label_train(self):\n",
    "        # Optional model configuration\n",
    "        model_args = MultiLabelClassificationArgs(num_train_epochs=2)\n",
    "        model_args.wandb_project = self.wandb_job\n",
    "        model_args.output_dir = self.output_dir\n",
    "\n",
    "\n",
    "        # Create a MultiLabelClassificationModel\n",
    "        model = MultiLabelClassificationModel(\n",
    "            \"bert\",\n",
    "            \"bert-base-chinese\",\n",
    "            num_labels=self.len,\n",
    "            args=model_args\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        model.train_model(self.train)\n",
    "\n",
    "        # Evaluate the model\n",
    "        result, model_outputs, wrong_predictions = model.eval_model(\n",
    "            self.test\n",
    "        )\n",
    "\n",
    "    \n",
    "    def mask_train(self):\n",
    "        # Configure the model\n",
    "        model_args = Seq2SeqArgs()\n",
    "        model_args.num_train_epochs = 2\n",
    "        model_args.evaluate_generated_text = True\n",
    "        model_args.output_dir = self.output_dir\n",
    "        model_args.wandb_project = self.wandb_job\n",
    "        model_args.evaluate_during_training = True\n",
    "        model_args.evaluate_during_training_verbose = True\n",
    "\n",
    "        model = Seq2SeqModel(\n",
    "            encoder_decoder_type='bart',\n",
    "            encoder_decoder_name='fnlp/bart-base-chinese',\n",
    "\n",
    "            args=model_args,\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        model.train_model(self.train, eval_data = self.eval)\n",
    "\n",
    "        # Evaluate the model\n",
    "        result = model.eval_model(self.test)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "data('mask_64.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "4afa11a65da338f3b8d43814e48d0d156a2819e630ba4c0a65e63d83e3f721f8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
