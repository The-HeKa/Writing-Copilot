{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-26 22:59:18.452053: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-26 22:59:18.694635: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-12-26 22:59:18.714310: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-26 22:59:18.714330: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-26 22:59:19.447668: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-26 22:59:19.447891: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-26 22:59:19.447899: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/yuan/anaconda3/envs/wtcpt/lib/python3.8/site-packages/ckiptagger/model_ws.py:106: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  cell = tf.compat.v1.nn.rnn_cell.LSTMCell(hidden_d, name=name)\n",
      "2022-12-26 22:59:22.936923: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-12-26 22:59:22.937248: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-26 22:59:22.937314: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-12-26 22:59:23.885025: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-26 22:59:23.908633: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled\n",
      "/home/yuan/anaconda3/envs/wtcpt/lib/python3.8/site-packages/ckiptagger/model_pos.py:56: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  cell = tf.compat.v1.nn.rnn_cell.LSTMCell(hidden_d, name=name)\n",
      "/home/yuan/anaconda3/envs/wtcpt/lib/python3.8/site-packages/ckiptagger/model_ner.py:57: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  cell = tf.compat.v1.nn.rnn_cell.LSTMCell(hidden_d, name=name)\n"
     ]
    }
   ],
   "source": [
    "import os, transformers\n",
    "import csv\n",
    "import spacy\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, BertTokenizerFast\n",
    "import opencc\n",
    "from ckiptagger import data_utils, construct_dictionary, WS, POS, NER\n",
    "import random\n",
    "\n",
    "converter = opencc.OpenCC('s2tw.json')\n",
    "\n",
    "# Download data\n",
    "# data_utils.download_data(\"./\")\n",
    "\n",
    "# Load model with CPU\n",
    "ws = WS(\"./data\")\n",
    "pos = POS(\"./data\")\n",
    "ner = NER(\"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['cls.predictions.decoder.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# cn_version\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "\n",
    "# zh-tw version\n",
    "# tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"bert-base-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate label\n",
    "\n",
    "def generate_label(sentence,tag_pos,p):\n",
    "\n",
    "    sentence_list = [sentence]\n",
    "    word_sentence_list = ws(sentence_list)\n",
    "    pos_sentence_list = pos(word_sentence_list)\n",
    "\n",
    "    label_list = []\n",
    "\n",
    "    for i in range(len(word_sentence_list)):\n",
    "        for r in range(len(word_sentence_list[i])):\n",
    "            word_ = word_sentence_list[i][r]\n",
    "            pos_ = pos_sentence_list[i][r]\n",
    "\n",
    "            label_list.append([word_,pos_])\n",
    "\n",
    "    # get tag's next word's position\n",
    "    def get_key(val):\n",
    "        result = []\n",
    "        position = 0\n",
    "        del_list = []\n",
    "        for i in range(len(label_list)):\n",
    "            key, value = label_list[i]\n",
    "            position += len(key)\n",
    "            \n",
    "            if val == value:\n",
    "\n",
    "                # judge del or save\n",
    "                # p = 0.9\n",
    "                p_ = random.randint(1,100)/100\n",
    "                if p_ <= p:\n",
    "                    # del\n",
    "                    position -= len(key)\n",
    "                    result.append([0,position])\n",
    "                    del_list.append(i)\n",
    "                    # print(0,position, sentence_list[0][position])\n",
    "                else:\n",
    "                    # save\n",
    "                    result.append([1,position])\n",
    "                    # print(1,position, sentence_list[0][position])\n",
    "        \n",
    "        # print(\"\".join([x[0] for x in label_list]))\n",
    "        del_list.sort(reverse=True)\n",
    "        for _ in del_list:\n",
    "            del label_list[_]\n",
    "        sent = \"\".join([x[0] for x in label_list])\n",
    "                    \n",
    "        return result, sent\n",
    "\n",
    "    label_tag, sent = get_key(tag_pos)\n",
    "    encoded_str = tokenizer(sent, padding=True, truncation=True) \n",
    "    tokens = tokenizer.convert_ids_to_tokens(encoded_str.input_ids)\n",
    "\n",
    "    lengh = len(tokens[1:-1])\n",
    "    label = ''\n",
    "\n",
    "    while len(label)<lengh:\n",
    "        label+='0'\n",
    "\n",
    "\n",
    "    for i in [r[1] for r in label_tag]:\n",
    "        label = list(label)\n",
    "        label[i] = '1'\n",
    "        label = ''.join(label)\n",
    "\n",
    "    # print(sentence_list[0])\n",
    "    # print(sent)\n",
    "\n",
    "    return sent, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('傅達仁今執行安樂死，卻突然爆出自己20年前遭緯來體育台封殺，他懂自己哪裡得罪到電視台。',\n",
       " '000010000001010000000000000000010000000000')"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_label('傅達仁今將執行安樂死，卻突然爆出自己20年前遭緯來體育台封殺，他不懂自己哪裡得罪到電視台。','D',0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wtcpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4afa11a65da338f3b8d43814e48d0d156a2819e630ba4c0a65e63d83e3f721f8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
